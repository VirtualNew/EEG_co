{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071ffac7-9617-4ad8-8df1-52a7b96576cf",
   "metadata": {},
   "source": [
    "## notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d5108-c0d4-4095-b424-4a9b26c9e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102f503-40ce-4132-b901-a2fcf8079f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "import warnings\n",
    "from transformers import TrainingArguments, Trainer, get_scheduler\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "torch.set_num_threads(12)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b226948-17c5-4aa4-8a81-06b61e93c425",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('../Data/data.json')\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d5b5d-3aed-4e31-b475-318b94d5e922",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./qwen/Qwen2-0___5B-Instruct', use_fast=False, trust_remote_code=True,torch_dtype=torch.bfloat16,device_map='balanced_low_0')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153f977-3548-4ad0-8069-497fecf35aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 100000  \n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    \n",
    "    # Tokenize the prompt and treatment\n",
    "    prompt = tokenizer(\n",
    "        f\"system\\nYou are an EEG emotion analyzer. I will input the patient's personal information and the EEG signals collected from some electrode positions. Please help me infer the patient's current emotion based on this signal. \\nuser\\n{example['prompt']}\\nassistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,  # Apply truncation at the tokenization step\n",
    "        max_length=MAX_LENGTH // 2  # Split the length allowance\n",
    "    )\n",
    "    \n",
    "    response = tokenizer(\n",
    "        f\"Emotion: {example['emotion_label']}. Treatment: {example['treatment']}.\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Construct input_ids, attention_mask, and labels\n",
    "    input_ids = prompt[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = prompt[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(prompt[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    \n",
    "    # Handle truncation if input exceeds MAX_LENGTH\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names, num_proc=12)  # Adjust num_proc as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28a3b3-4688-42d2-9d18-55e6a00e70b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3325fb-487b-4dd6-9fdb-8a7abbf5b8b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514a730-3af3-444c-94f3-9a03a6f1ccf2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig, BitsAndBytesConfig\n",
    "model = AutoModelForCausalLM.from_pretrained('/root/.cache/modelscope/hub/qwen/Qwen2-0.5B-Instruct', device_map=\"balanced_low_0\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72189489-1789-4c65-9ba3-d39f7a6eb7ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    './qwen/Qwen2-0___5B-Instruct',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cuda:0',\n",
    "    trust_remote_code=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "from collections import defaultdict\n",
    "\n",
    "param_counts = defaultdict(int)\n",
    "total_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # for example, \"mlp.gate_proj\"„ÄÅ\"self_attn.q_proj\"\n",
    "    parts = name.split(\".\")\n",
    "    if \"mlp\" in parts:\n",
    "        key = \".\".join([p for p in parts if p in {\"mlp\", \"gate_proj\", \"up_proj\", \"down_proj\"}])\n",
    "    elif \"self_attn\" in parts:\n",
    "        key = \".\".join([p for p in parts if p in {\"self_attn\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"}])\n",
    "    elif \"layernorm\" in name:\n",
    "        key = \"layernorm\"\n",
    "    elif \"embed_tokens\" in name:\n",
    "        key = \"embed_tokens\"\n",
    "    elif \"norm\" in name:\n",
    "        key = \"final_norm\"\n",
    "    else:\n",
    "        key = \"others\"\n",
    "\n",
    "    param_counts[key] += param.numel()\n",
    "    total_params += param.numel()\n",
    "\n",
    "# Output in descending order by parameter count.\n",
    "print(f\"{'Module Type':<30} : Parameters\")\n",
    "print(\"-\" * 50)\n",
    "for module, count in sorted(param_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{module:<30} : {count:,}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe0c77-a538-45b8-aa9f-14f3ef2306cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # When enabling gradient checkpointing, you need to call this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dca344-f91f-4f98-94a6-c742bdaf15e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ce4b1-7af6-47ff-99b4-71bfac90da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b7493-83d9-4489-bdb9-fdac20386dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # Training Mode\n",
    "    r=8, # Lora rank\n",
    "    lora_alpha=32, # Lora alaph\n",
    "    lora_dropout=0.5# Dropout \n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0aed7-9726-45a8-be09-1e20b9342838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a74f48-3823-4ab8-8362-4a37416d66fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a6e7d-e85d-4a6b-96fa-0deb601b62e0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./qwem2_0.5b_new/exp1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=6,\n",
    "    save_steps=3850,\n",
    "    learning_rate=1e-5,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    weight_decay=0.01,  \n",
    "    max_grad_norm=2.0,  \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    dataloader_num_workers=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90ef0a-7bfb-4252-a899-6b2c44972a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db8bd4-962d-4a41-9f2a-7e22d27e754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learning rate scheduler\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=len(tokenized_id) // args.per_device_train_batch_size * args.num_train_epochs)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=trainer.optimizer,\n",
    "    num_warmup_steps=100,  # Set the number of learning rate warmup steps.\n",
    "    num_training_steps=len(tokenized_id) // args.per_device_train_batch_size * args.num_train_epochs,\n",
    ")\n",
    "\n",
    "trainer.lr_scheduler = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20463028-64c9-4ebe-98a9-512fe3530593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251c32c-acab-48c8-ab7e-435dad42b080",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa947a0-9830-4d21-8b3d-3e0116ec13c1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model merging and saving\n",
    "new_model_directory = \"./merged_model_qwen2_0.5b_new1/\"\n",
    "merged_model = model.merge_and_unload()\n",
    "# Save the weights in safetensors format, with each weight file not exceeding 2 GB (2048 MB).\n",
    "merged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6efba9-0d26-4e5c-908b-df42ea78c5c8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /root/.cache/modelscope/hub/qwen/Qwen2-0.5B-Instruct/config.json ./merged_model_qwen2_0.5b_new1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd2323-d732-4e15-963b-c51e1975401f",
   "metadata": {},
   "source": [
    "## Restart notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4e63f-1488-4417-8060-b68a65b64803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization as quantization\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "model_path='./merged_model'\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float, trust_remote_code=True).eval()\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# Set quantization configuration for the model's embedding layer\n",
    "model.embeddings.qconfig = float_qparams_weight_only_qconfig\n",
    "\n",
    "# Prepare the model for quantization\n",
    "model_prepared = quantization.prepare(model, inplace=False)\n",
    "\n",
    "# Define test input data and ensure it is on CPU\n",
    "input_text = \"This is a test sentence.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "inputs = {k: v.to('cpu') for k, v in inputs.items()}\n",
    "\n",
    "# Simulate forward pass during Quantization-Aware Training (QAT)\n",
    "with torch.no_grad():\n",
    "    outputs = model_prepared(**inputs)\n",
    "\n",
    "# Convert the model to its quantized version\n",
    "model_quantized = quantization.convert(model_prepared, inplace=False)\n",
    "\n",
    "# Print the structure of the quantized model\n",
    "print(\"Quantized model architecture:\")\n",
    "print(model_quantized)\n",
    "\n",
    "# Test the quantized model\n",
    "with torch.no_grad():\n",
    "    quantized_outputs = model_quantized(**inputs)\n",
    "\n",
    "# Print the output of the quantized model\n",
    "print(\"Quantized output:\", quantized_outputs.last_hidden_state)\n",
    "\n",
    "# Save the quantized model\n",
    "int8_model_directory='INT8'\n",
    "save_pretrained(int8_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f14a08-5db9-4f11-88b8-638647a8bfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
